\makeproblem{}{problem:multiplication:2dvectorsquare}{
Generalize the calculation of \cref{eqn:gaTutorial:80} to calculate the square of an \R{n} vector.

\begin{dmath}\label{eqn:multiplication:100}
\Bx = \sum_i x_i \Be_i
\end{dmath}
} % problem

\makeanswer{problem:multiplication:2dvectorsquare}{
Consider the 2D case to start with

\begin{dmath}\label{eqn:multiplication:120}
\Bx^2
=
\lr{ x \Be_1 + y \Be_2}
\lr{ x \Be_1 + y \Be_2}
=
\lr{ x \Be_1 } \lr{ x \Be_1 }
+
\lr{ y \Be_2 } \lr{ y \Be_2 }
+
\lr{ x \Be_1 } \lr{ y \Be_2 }
+
\lr{ y \Be_2 } \lr{ x \Be_1 }
=
x^2 \Be_1^2
+
y^2 \Be_2^2
+
x y \lr{ \Be_1 \Be_2 + \Be_2 \Be_1 }
=
x^2 + y^2
+
x y \lr{ \Be_1 \Be_2 + \Be_2 \Be_1 }.
\end{dmath}

The contraction axiom requires the bivector terms to sum to zero, as also demonstrated previously for the specific example \( \Bx = \Be_1 + \Be_2 \).

More generally for \R{N}

\begin{dmath}\label{eqn:multiplication:121}
\Bx^2
=
\lr{ \sum_i x_i \Be_i }
\lr{ \sum_j x_j \Be_j }
=
\sum_{ij} x_i x_j \Be_i \Be_j
=
\sum_{i = j} x_i x_j \Be_i \Be_j
+
\sum_{i \ne j} x_i x_j \Be_i \Be_j
=
\sum_{i} x_i^2
+
\sum_{i \ne j} x_i x_j \Be_i \Be_j
=
\sum_{i} x_i^2
+
\sum_{i < j} x_i x_j (\Be_i \Be_j + \Be_j \Be_i).
\end{dmath}

The contraction axiom requires all the bivector pairs to sum to zero.  That is, for each \( i \ne j \)

\begin{dmath}\label{eqn:introGAproblems:140}
\Be_i \Be_j = -\Be_j \Be_i.
\end{dmath}
} % answer
