A new student of vector algebra will first learn 
%first see vectors in two or three dimensions as sets of coordinates
%
%\begin{equation}\label{eqn:GAmotivation:20}
%\Ba = 
%\begin{bmatrix}
%a_1 \\
%a_2 \\
%a_3 \\
%\end{bmatrix}, \qquad
%\Bb = 
%\begin{bmatrix}
%b_1 \\
%b_2 \\
%b_3 \\
%\end{bmatrix},
%\end{equation}
%
%or perhaps explicitly in terms of a basis \( \setlr{ \Be_1, \Be_2 \Be_3 } \)
%
%\begin{dmath}\label{eqn:GAmotivation:40}
%\begin{aligned}
%\Ba &= a_1 \Be_1 + a_2 \Be_2 + a_3 \Be_3  \\
%\Bb &= b_1 \Be_1 + b_2 \Be_2 + b_3 \Be_3
%\end{aligned}.
%\end{dmath}
%
%You will learn the 
rules for addition and subtraction of such vectors.  
%, and then how to operate on them with rotation matrices or other representations of linear transformations.  
This demonstrates to the student that the vector is an algebraic object that generalize numbers, and the question of how to 
multiply vectors soon follows.

Given the toolbox of traditional vector algebra, the best answer that a new student will obtain from such a line of questioning is to learn of the dot and cross products, multiplication like operations that we are all so familiar with

%\begin{itemize}
%\item ``You can not multiply vectors.'', or
%\item ``Vector multiplication is not well defined.'', or
%\item ``We will get to that.'', or
%\item ``There are multiplication like operations.''  The 
%\end{itemize}

\begin{dmath}\label{eqn:GAmotivation:60}
\begin{aligned}
\Ba \cdot \Bb &= a_1 b_1 + a_2 b_2 + a_3 b_3 = \Abs{\Ba} \Abs{\Bb} \cos \theta_{ab} \\
\Ba \cross \Bb &= 
\begin{vmatrix}
\Be_1 & \Be_2 & \Be_3 \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
\end{vmatrix}
= \ncap_{ab} \Abs{\Ba} \Abs{\Bb} \sin\theta_{ab}.
\end{aligned}
\end{dmath}

Both of these multiplication like operations live in very different spaces, one scalar, and the other a vector that lies outside of the span of its two vector factors.  Observe that the magnitudes of these two product operations are related to the product of the vectors in a Pythagorean sense

\begin{dmath}\label{eqn:GAmotivation:180}
\lr{ \Ba \cdot \Bb }^2 + \lr{ \Ba \cross \Bb }^2 
=
\Abs{\Ba}^2 \Abs{\Bb}^2 \cos^2 \theta_{ab} 
+\Abs{\Ba}^2 \Abs{\Bb}^2 \sin^2 \theta_{ab} 
= 
\Abs{\Ba}^2 \Abs{\Bb}^2.
\end{dmath}

This can be seen as a hint that the dot and cross products might be components of a single vector product operation, but the precise form of that product is not obvious.

Vector products that have the same form as the scalar magnitudes of the dot and cross products can be found in other algebraic systems.  Given a complex number representation of two vectors in a 2D space

\begin{dmath}\label{eqn:GAmotivation:200}
\begin{aligned}
z &= r e^{i \theta} \leftrightarrow (a, b) \\
w &= \rho e^{i \alpha} \leftrightarrow (a', b'),
\end{aligned}
\end{dmath}

the inner product of such a complex vector representation can be seen to have the same structure as the dot and cross products

\begin{equation}\label{eqn:GAmotivation:100}
\begin{aligned}
\Real( z w^\conj ) &= r \rho \cos(\theta - \alpha) \\
\Imag( z w^\conj ) &= r \rho \sin(\theta - \alpha).
\end{aligned}
\end{equation}

One can readily show that this inner product has the following vector isomorphism

\begin{dmath}\label{eqn:GAmotivation:220}
z w^\conj \leftrightarrow ( a a' + b b', a' b - a b' ).
\end{dmath}

One component is completely symmetric, whereas the other component of this product has a component that is completely antisymmetric.  
The 3D cross product also has this antisymmetry, and that antisymmetry will be seen later to be the key to the generalization of the cross product.  In this particular case, one can view this antisymmetric sum \( a' b - a b' \) as one 
answer of how the cross product ``generalizes'' from 3D to 2D without requiring the introduction of a normal dimension.

The answer to questions of exactly how the vector products, in particular the cross product, should generalize to higher dimensional spaces is still outstanding.  It should be expected that this cross product generalization will involve antisymmetry, just as the dot product generalization in higher dimensional spaces is completely symmetric.

Many current students of science may never see the exact structure of this generalization.  Should studies happen to include 
enough of the right esoteric physics and mathematics (quantum mechanics, QED, calculus on manifolds, ...) then 
some answers to those questions may be found.  Unfortunately, there are many such answers, and many of them each only provide 
one part of the picture.

For example, a student of non-relativistic quantum mechanics will learn of Pauli matrices.  The dot and cross products will be seen to be components of a more general vector multiplication operation

\begin{equation}\label{eqn:GAmotivation:120}
\lr{\Bsigma \cdot \Bx }
\lr{\Bsigma \cdot \By }
=
I \lr{ \Bx \cdot \By } + i \Bsigma \cdot \lr{ \Bx \cross \By }.
\end{equation}

A student of quantum field theory will encounter Dirac matrices, a algebraic structure that allows for the multiplication of four-vectors

\begin{dmath}\label{eqn:GAmotivation:140}
\aslash \bslash
=
\inv{2} \symmetric{ \aslash}{ \bslash }
+
\inv{2} \antisymmetric{ \aslash}{ \bslash }
=
a^\mu b_\mu + \inv{2} a^\mu b^\nu \antisymmetric{\gamma_\mu}{\gamma_\nu}
=
a^\mu b_\mu + \inv{2} a^\mu b^\nu \lr{ 
\gamma_\mu \gamma_\nu
- 
\gamma_\nu \gamma_\mu
}.
\end{dmath}

A product of ``Dirac'' vectors has symmetric and antisymmetric components that generalize the dot and cross products.  
Unfortunately, this algebra comes with still another different notation.
One interesting take away from this particular vector product is the fact that one component is a scalar, and other other 
involves products of vectors, something that will require further interpretation.  Since the Dirac basis typically has a matrix representation, such a product can be dismissed as just being another matrix.  The products of mutually orthonormal vectors will show up again later in a context where there is no requirement to assume a matrix representation of the underlying basis.

There are still other algebraic systems, such as quaternion algebra, where the dot and cross products will be found in.  

Another common and important context that contains generalizations of the dot and cross products is the subject of differential forms.
A student of differential forms will learn how to compute the wedge products of forms, and of duality operations, which can be used to construct generalized multiplication operations that have the structure of the 3D dot and cross products

\begin{equation}\label{eqn:GAmotivation:160}
\begin{aligned}
df \wedge * dg &= \lr{ \sum_{i=1}^3 \PD{x_i}{f} \PD{x_i}{g} } dx_1 \wedge dx_2 \wedge dx_3 \\
df \wedge dg &= \sum_{1 \le i < j \le 3} \lr{
\PD{x_i}{f} \PD{x_j}{g} 
-\PD{x_j}{f} \PD{x_i}{g} 
}
dx_i \wedge dx_j.
\end{aligned}
\end{equation}

It is possible to express vectors as a differential form, and some advocate for this \citep{flanders1989dfa}, but this can also seem unnatural.  Regardless, differential forms do highlight the existance of more general concepts of vector multiplication.  %In this particular case, this generality comes with the cost of using yet another notation, one that is considerably different than the vector notation that we are comfortable with.  

It should not be surprising that all of these ideas are special cases of a more general algebraic system.

The aim of the material to follow is to provide the instruction manual for an enhanced toolbox of vector algebra techniques that can be used to gain an integrated view of many seemingly disparate mathematical methods.  These are tools that can be learned without having to first study the esoteric arts of quantum mechanics or differential forms, and have many applications once learned.  These notes will focus on applications to the study of electromagnetism.

