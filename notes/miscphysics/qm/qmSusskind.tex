%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

% 
% 
%\documentclass{article}

%\input{../peeters_macros.tex}
%\input{../peeters_macros2.tex}
%\usepackage[bookmarks=true]{hyperref}

%\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
%\usepackage{latexsym,amssymb,epsf} % do not remember if these are
   % needed, but their inclusion can not do any damage


\chapter{Notes on Susskind's QM Lecture}
\label{chap:PJQmSusskind}
%\author{Peeter Joot \quad peeter.joot@gmail.com}
\date{ Dec 23, 2008.  qmSusskind.tex }

%\begin{document}

%\maketitle{}

%\tableofcontents

\section{Motivation}

Some notes on Susskind lectures.  Mostly of lecture 3 where the postulates are laid out, after some of the preliminaries have been established.

\section{Bra and Ket vectors}

An odd looking vector notation is introduced.  Instead of just using a letter, say \(A\), for a vector in \C{N}, such a
vector is instead written

\begin{equation}\label{eqn:qmSusskind:20}
\begin{aligned}
\ket{A}
\end{aligned}
\end{equation}

This is called a ``ket'' or ket-vector, but really just means complex vector.   The complex conjugate of this
vector is then written as a ``bra'' like so

\begin{equation}\label{eqn:qmSusskind:40}
\begin{aligned}
\bra{A}
\end{aligned}
\end{equation}

The inner product of two vectors can then be written by combining this bra and ket by butting them up together, as
in

\begin{equation}\label{eqn:qmSusskind:60}
\begin{aligned}
\braket{A}{B}
\end{aligned}
\end{equation}

Contrast this to the explicit complex column vector representation

\begin{equation}\label{eqn:qmSusskind:80}
\begin{aligned}
{A} = 
\begin{bmatrix}
a_1 \\
a_2 \\
\vdots \\
a_n
\end{bmatrix}
\quad
{B} = 
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
\end{aligned}
\end{equation}

in a finite dimensional space.  The usual convention is to employ an inner product notation like

\begin{equation}\label{eqn:qmSusskind:100}
\begin{aligned}
\innerprod{A}{B} = {A}^{\text{T}} \overbar{B} = \sum_i {a_i} \overbar{b_i}
\end{aligned}
\end{equation}

or,
\begin{equation}\label{eqn:qmSusskind:120}
\begin{aligned}
\innerprod{A}{B} = {A}^\conj {B} = \sum_i \overbar{a_i} {b_i}
\end{aligned}
\end{equation}

Observe that the braket notation is closer to this last form with the conjugation on the first term.  However, note that the metric associated with the braket notation has not been specified yet.  This is in fact an integral over space, where the ket vectors are complex valued functions.

Note that the vectors of QM are typically complex functions, and in the continuous case the inner product representation is

\begin{equation}\label{eqn:qmSusskind:140}
\begin{aligned}
\braket{\psi}{\phi} = \int {\psi}^\conj {\phi}
\end{aligned}
\end{equation}

There appears to be some flexibility in choosing this integral, and its range.  Specific examples used in the lectures are

\begin{equation}\label{eqn:qmSusskind:160}
\begin{aligned}
\braket{\psi}{\phi} &= \int_{x=-\infty}^{x=\infty} {\psi(x)}^\conj {\phi(x)} dx \\
\braket{\psi}{\phi} &= \int {\psi(x,y,z)}^\conj {\phi(x,y,z)} dx dy dz \\
\braket{\psi}{\phi} &= \int_{x=0}^{2\pi} {\psi(x)}^\conj {\phi(x)} dx \\
\end{aligned}
\end{equation}

The last was used in the circular motion treatment of lecture 4. 

\subsection{Coordinates and basis notation}

Ket vectors represent states, and the labels that are used for these are pretty loose.  For example, instead
of writing the n'th basis vector as 

\begin{equation}\label{eqn:qmSusskind:180}
\begin{aligned}
\ket{a_n}
\end{aligned}
\end{equation}

just n was used like so

\begin{equation}\label{eqn:qmSusskind:200}
\begin{aligned}
\ket{n}
\end{aligned}
\end{equation}

so if \(\{\ket{n}\}\) is a basis, the coordinates of a vector \(\ket{A}\) can be written

\begin{equation}\label{eqn:qmSusskind:220}
\begin{aligned}
\ket{A} = \sum_n \alpha_n \ket{n}
\end{aligned}
\end{equation}

Note that here in the summation sign, and the subscript \(n\) is an index, and also implicitly indices the basis vectors, but in that context is not a number but a label for the basis vector itself.

Assuming that the \(\ket{n}\) vectors are orthonormal, we can take inner products (brackets) to compute the \(\alpha_n\) coordinates.

Writing \(\ket{k}\) as an alternate labeling for the same basis, the conjugate bra vectors when sandwiched against this bra representation denotes the inner product

\begin{equation}\label{eqn:qmSusskind:240}
\begin{aligned}
\braket{n}{A} 
&= \sum_k \alpha_k \braket{n}{k} \\
&= \sum_k \alpha_k \delta_{nk} \\
&= \alpha_n \\
\end{aligned}
\end{equation}

So we have

\begin{equation}\label{eqn:qmSusskind:260}
\begin{aligned}
\ket{A} = \sum_n \braket{n}{A} \ket{n}
\end{aligned}
\end{equation}

Since \(\ket{n}\) is a vector and \(\braket{n}{A}\) is just a complex number, this can be rearranged to butt the points
together as a mnemonic reminder that this is a projective operation.

\begin{equation}\label{eqn:qmSusskind:280}
\begin{aligned}
\ket{A} = \sum_n \ket{n} \braket{n}{A} 
\end{aligned}
\end{equation}

As is the case with an orthonormal split by projection matrices, this can be observed to be more than a memory
device since the object

\begin{equation}\label{eqn:qmSusskind:300}
\begin{aligned}
\ketbra{n}{n}
\end{aligned}
\end{equation}

is in fact the orthonormal projection operator onto the \(\ket{n}\) direction.  ie:

\begin{equation}\label{eqn:qmSusskind:320}
\begin{aligned}
\Proj_{\ket{n}}(\ket{A}) = \left( \ketbra{n}{n} \right) \ket{A}
\end{aligned}
\end{equation}

Note this is not summed over indices \(n\).

In the matrix representation, this is really nothing more than writing

\begin{equation}\label{eqn:qmSusskind:340}
\begin{aligned}
\Proj_{e} (A) = e \innerprod{e}{A} = e (e^\conj A) = (e e^\conj) A
\end{aligned}
\end{equation}

So one can think of this funny looking \(\ketbra{n}{n}\) as nothing more than the orthonormal projector matrix of the form \(e e^\conj\).

\subsection{Dual space}

Susskind called the set of the conjugate vectors (the bras), the dual space.  If the basis is not orthonormal
are the conjugates really the duals (reciprocals) of the basis vectors?  Let us see with an example:

\begin{equation}\label{eqn:qmSusskind:360}
\begin{aligned}
\ket{1} = 
\inv{\sqrt{2}}
\begin{bmatrix}
1 \\
i
\end{bmatrix}
, \quad
\ket{2} = 
\inv{\sqrt{5}}
\begin{bmatrix}
i \\
2
\end{bmatrix}
\end{aligned}
\end{equation}

Here we have

\begin{equation}\label{eqn:qmSusskind:380}
\begin{aligned}
\braket{1}{1} = 
\inv{2}
\begin{bmatrix}
1 & -i
\end{bmatrix}
\begin{bmatrix}
1 \\
i
\end{bmatrix}
= 1
\end{aligned}
\end{equation}

and 

\begin{equation}\label{eqn:qmSusskind:400}
\begin{aligned}
\braket{2}{2} = 
\inv{5}
\begin{bmatrix}
-i & 2 
\end{bmatrix}
\begin{bmatrix}
i \\
2
\end{bmatrix}
= 1
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:qmSusskind:420}
\begin{aligned}
\braket{1}{2} = 
\inv{\sqrt{10}}
\begin{bmatrix}
1 & -i
\end{bmatrix}
\begin{bmatrix}
i \\
2
\end{bmatrix}
= -\frac{i}{\sqrt{10}}
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:qmSusskind:440}
\begin{aligned}
\braket{2}{1} = 
\inv{\sqrt{10}}
\begin{bmatrix}
-i & 2
\end{bmatrix}
\begin{bmatrix}
1 \\
i
\end{bmatrix}
= \frac{i}{\sqrt{10}}
\end{aligned}
\end{equation}

Definitely not the dual space.  The conjugates are only going to be the dual basis when the primary basis is orthonormal.

Switching back to abstraction temporarily, let us calculate the coordinates with respect to a non-orthonormal
basis of dimension \(k\).  That is, determine the \(\alpha_i\) given a decomposition by basis vectors

\begin{equation}\label{eqn:qmSusskind:460}
\begin{aligned}
\ket{x} = \sum_i \alpha_i \ket{a_i}
\end{aligned}
\end{equation}

We have 
\begin{equation}\label{eqn:qmSusskind:480}
\begin{aligned}
\braket{a_j}{x} 
&= \sum_i \alpha_i \braket{a_j}{a_i} \\
&=
\begin{bmatrix}
\braket{a_j}{a_1} & \braket{a_j}{a_2} & \hdots & \braket{a_j}{a_k}
\end{bmatrix}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\alpha_k \\
\end{bmatrix}
\end{aligned}
\end{equation}

Assembling these into a matrix with a column for each \(j\), we have
\begin{equation}\label{eqn:qmSusskind:500}
\begin{aligned}
\begin{bmatrix}
\braket{a_1}{x} \\
\braket{a_2}{x} \\
\vdots \\
\braket{a_k}{x} \\
\end{bmatrix}
&=
{\begin{bmatrix}
\braket{a_i}{a_j}
\end{bmatrix}}_{ij}
\Balpha
\end{aligned}
\end{equation}

With,

\begin{equation}\label{eqn:qmSusskind:520}
\begin{aligned}
A = 
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix},
\end{aligned}
\end{equation}

this is

\begin{equation}\label{eqn:qmSusskind:540}
\begin{aligned}
\Balpha = \inv{A^\conj A} A^\conj \ket{x}
\end{aligned}
\end{equation}

or 
\begin{equation}\label{eqn:qmSusskind:560}
\begin{aligned}
\ket{x} = A \Balpha = A \inv{A^\conj A} A^\conj \ket{x}
\end{aligned}
\end{equation}

From this we can pick off the reciprocal frame vectors, which are the columns of 

\begin{equation}\label{eqn:qmSusskind:580}
\begin{aligned}
\begin{bmatrix}
\ket{a^1} & \ket{a^2} & \hdots & \ket{a^k}
\end{bmatrix} = 
A \inv{A^\conj A}
\end{aligned}
\end{equation}

To verify we calculate 
\begin{equation}\label{eqn:qmSusskind:600}
\begin{aligned}
{\begin{bmatrix}
\ket{a^1} & \ket{a^2} & \hdots & \ket{a^k}
\end{bmatrix}}^\conj
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix}
&=
\begin{bmatrix}
\bra{a^1} \\ \bra{a^2} \\ \vdots \\ \bra{a^k}
\end{bmatrix}
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix} \\
&= 
{
\begin{bmatrix}
\braket{a^i}{a_j}
\end{bmatrix}}_{ij}
\end{aligned}
\end{equation}

with the expectation that this is the identity matrix.  That product is

\begin{equation}\label{eqn:qmSusskind:620}
\begin{aligned}
&\left(
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix}
\inv{{\begin{bmatrix}
\braket{a_i}{a_j}
\end{bmatrix}}_{ij}} \right)^\conj
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix} \\
&=
\inv{{\begin{bmatrix}
\braket{a_j}{a_i}^\conj
\end{bmatrix}}_{ij}} 
\begin{bmatrix}
\bra{a_1} \\ \bra{a_2} \\ \vdots \\ \bra{a_k}
\end{bmatrix}
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix} \\
&=
\inv{{\begin{bmatrix}
\braket{a_i}{a_j}
\end{bmatrix}}_{ij}} 
{{\begin{bmatrix}
\braket{a_i}{a_j}
\end{bmatrix}}_{ij}} \\
&= I
\end{aligned}
\end{equation}

This proves the desired result, that we can calculate \(\braket{a^i}{a_j} = \delta_{ij}\) where 

\begin{equation}\label{eqn:qmSusskind:640}
\begin{aligned}
\begin{bmatrix}
\ket{a^1} & \ket{a^2} & \hdots & \ket{a^k}
\end{bmatrix} = 
\begin{bmatrix}
\ket{a_1} & \ket{a_2} & \hdots & \ket{a_k}
\end{bmatrix}
\inv{{\begin{bmatrix}
\braket{a_i}{a_j}
\end{bmatrix}}_{ij}}
\end{aligned}
\end{equation}

While kind of fun to see how to express this in the bra ket notation, is this useful.  Probably not since
all the operators of QM are Hermitian, and thus have orthonormal basis (the eigenvectors).  Oh well... it has
provided some comfort with the notation if nothing else.

\subsection{Hermitian operators}

The operators of QM are written with hats, and are applied to vectors (states).  For example for an 
operator \(\hatH\) applied to \(\ket{x}\) we write

\begin{equation}\label{eqn:qmSusskind:660}
\begin{aligned}
\hatH \ket{x}
\end{aligned}
\end{equation}

FIXME: Express op applied to a vector in coordinates.

Additionally, the inner product is written with a sandwich bra ket format like

\begin{equation}\label{eqn:qmSusskind:680}
\begin{aligned}
\braket{y}{ \left(\hatH \ket{x} \right) }
=
\BraOpKet{y}{H}{x}
\end{aligned}
\end{equation}

In the braket notation a Hermitian operator \(\hatH\) is defined as one 

\begin{equation}\label{eqn:qmSusskind:700}
\begin{aligned}
\BraOpKet{A}{H}{B}^\conj = \BraOpKet{B}{H}{A}
\end{aligned}
\end{equation}

whereas an anti-Hermitian (or skew-Hermitian) operator is:

\begin{equation}\label{eqn:qmSusskind:720}
\begin{aligned}
\BraOpKet{A}{S}{B}^\conj = -\BraOpKet{B}{S}{A}
\end{aligned}
\end{equation}

FIXME: expand \(\hatH \ket{A}\) in coordinates and then the inner product of that with another vector.

\subsubsection{Tricky eigenvalue notation}

Worth a small note is a comment on some tricky notation used for eigenvectors and values.  He writes

\begin{equation}\label{eqn:qmSusskind:740}
\begin{aligned}
\hatH \ket{\mathLabelBox{\lambda}{A}} = \mathLabelBox{\lambda}{B} 
\mathLabelBox
[
   labelstyle={below of=m\themathLableNode, below of=m\themathLableNode}
]
{\ket{\lambda}}{C}
\end{aligned}
\end{equation}

In the A case, \(\lambda\) is just a label.  In the B case, \(\lambda\) is a (real) number, and in the C case the whole thing \(\ket{\lambda}\) is a vector or state.  We have \(\lambda\) overloaded for labeling, a numeric value, and a vector, and one uses the context to distinguish these.

\section{Postulates of QM}

Rather than starting with the Schr\"{o}dinger equation or an attempt to motivate it, Susskind, after bra/ket preliminaries, starts with an axiomatic approach that I found unexpected.  It sounds like this is how Dirac laid it out in the thirties.  These postulates, if I jotted them down right, were

\begin{enumerate}

\item State vectors are orthogonal (can be made to be orthonormal), and describe distinct configurations.  An example of possible configurations are all the (infinite number of) positions that a particle can take in space.

\item The observables of a system are the collection of Hermitian operators.

\item The values (that which can be obtained by measurement) of observable \(\hatH\) are the eigenvalues of that operator.  Since \(\hatH\) is
Hermitian these eigenvalues are real (ie: measurements are real numbers).

\item States for which observable \(\hatH\) is certain, definite, and/or not statistical are the eigenvectors of \(\hatH\).

As an example, thinking back to lecture I or II he discusses a heads/tails system and says that QM unlike classical mechanics allows for
a linear combination of heads and tails states such as

\begin{equation}\label{eqn:qmSusskind:760}
\begin{aligned}
S = \alpha \ket{H} + \beta \ket{T}
\end{aligned}
\end{equation}

provided that the additional constraint \(S S^\conj = \alpha{\alpha}^\conj + \beta{\beta}^\conj = 1\) is imposed.  Here the combined state is a representation
of the probabilities for the two non-statistical states heads and tails.

\item Probability of eigenstate \(\lambda\) given state A is

\begin{equation}\label{eqn:qmSusskind:780}
\begin{aligned}
P_{\lambda}
&= \Abs{\braket{\lambda}{A}}^2 \\
&= {\braket{\lambda}{A}} {\braket{A}{\lambda}} \\
&= \braket{A}{\lambda}\braket{\lambda}{A}
\end{aligned}
\end{equation}

\end{enumerate}

\section{Position operator}

The position operator is defined as

\begin{equation}\label{eqn:qmSusskind:800}
\begin{aligned}
\hatx \ket{\psi} \rightarrow x \phi(x)
\end{aligned}
\end{equation}

This is a representation of the operator \(\hatx\) applied to ket vector \(\ket{\phi}\).  Applied in an inner product this operator produces the
following action

\begin{equation}\label{eqn:qmSusskind:820}
\begin{aligned}
\BraOpKet{\phi}{x}{\phi} \rightarrow \int {\phi^\conj}(x) x \phi(x) dx
\end{aligned}
\end{equation}

which shows that \(\hatx\) is Hermitian.

Eigenvectors and values of this operator in the continuous case are actually delta functions.  The eigen equation is

\begin{equation}\label{eqn:qmSusskind:840}
\begin{aligned}
\hatx \ket{\phi} = \lambda \ket{\phi}
\end{aligned}
\end{equation}

Or in the function representation

\begin{equation}\label{eqn:qmSusskind:860}
\begin{aligned}
x \phi_{\lambda}(x) = \lambda \phi_{\lambda}(x)
\end{aligned}
\end{equation}

or
\begin{equation}\label{eqn:qmSusskind:880}
\begin{aligned}
(x - \lambda) \phi_{\lambda}(x) = 0
\end{aligned}
\end{equation}

Susskind's argument that these eigenfunctions \(\phi_\lambda\) are Dirac delta functions is that for any \(x\ne \lambda\), \(\phi_\lambda(x) = 0\), and that at \(x = \lambda\) \(\phi_\lambda(x) can be non-zero\).  As Susskind puts it ``zero everywheres except at \(x = \lambda\)''.  The eigenvalues are all possible values of \(x\).

FIXME: Now, where does the unit integral come from in this argument?

A pictorial illustration of the delta function was given with a \(\epsilon\) width rectangle of height \(1/epsilon\).

orthonormal: \(\braket {\delta_{\lambda}(x)} {\delta_{\lambda'}(x)} = 0\) for \(\lambda \ne \lambda'\).  FIXME: fill in the details.

The mapping between state and function notation is

\begin{equation}\label{eqn:qmSusskind:900}
\begin{aligned}
\braket{x}{\phi} \rightarrow \int \delta(u-x) \phi(u) du = \phi(x)
\end{aligned}
\end{equation}

and the probability to locate particle at \(x\) from the postulates is thus

\begin{equation}\label{eqn:qmSusskind:920}
\begin{aligned}
\Prob_x(\phi) = \Abs{\braket{x}{\phi}}^2 = \int \phi^\conj \phi
\end{aligned}
\end{equation}

FIXME: fill in the details for both of these.

\section{Derivative (momentum) operator}

The next operator that is introduced is the skew-Hermitian operator \(\PDi{x}{}\).  Instead of showing that this was the case his preference was
to instead show that 

\begin{equation}\label{eqn:qmSusskind:940}
\begin{aligned}
\hatk = - i \PD{x}{}
\end{aligned}
\end{equation}

is in fact Hermitian.  To see this we need to apply this operator in an inner product context

\begin{equation}\label{eqn:qmSusskind:960}
\begin{aligned}
\BraOpKet{\phi}{k}{\phi} 
&= \int \phi^\conj \left( - i \PD{x}{} \right) \phi dx \\
\end{aligned}
\end{equation}

Integrating by parts one has (with \(\phi = 0\) on the boundaries) we have
\begin{equation}\label{eqn:qmSusskind:980}
\begin{aligned}
\BraOpKet{\phi}{k}{\phi} 
&= \int \PD{x}{\phi^\conj} i \phi dx \\
\end{aligned}
\end{equation}

But we also have

\begin{equation}\label{eqn:qmSusskind:1000}
\begin{aligned}
\BraOpKet{\phi}{k}{\phi}^\conj = 
&= \int \phi\left( i \PD{x}{} \right) \phi^\conj dx \\
\end{aligned}
\end{equation}

which completes the demonstration that \(\hatk\) is Hermitian

\begin{equation}\label{eqn:qmSusskind:1020}
\begin{aligned}
\BraOpKet{\phi}{k}{\phi}^\conj = \BraOpKet{\phi}{k}{\phi}.
\end{aligned}
\end{equation}

The eigenvalues and eigenfunctions follow directly

\begin{equation}\label{eqn:qmSusskind:1040}
\begin{aligned}
-i \PD{x}{\phi} &= k \phi \\
\PD{x}{\phi} &= i k \phi
\end{aligned}
\end{equation}

So we have for eigenvalue \(k\) the eigenfunction \(\phi_k(x) = e^{i k x}\).  Compare this to the \(\hatx\) eigenfunctions which are completely peaked.  Here the \(\hatk\) eigenfunctions are totally spread.

\subsection{wave number to momentum}

For wave function

\begin{equation}\label{eqn:qmSusskind:1060}
\begin{aligned}
\phi = e^{ikx}
\end{aligned}
\end{equation}

In terms of wavelength \(L\), this is

\begin{equation}\label{eqn:qmSusskind:1080}
\begin{aligned}
k = \frac{2\pi}{L}
\end{aligned}
\end{equation}

Susskind writes the DeBroglie hypothesis as 

\begin{equation}\label{eqn:qmSusskind:1100}
\begin{aligned}
p 
&= \frac{h}{L} \\
&= \frac{h}{2\pi/k} \\
&= {\Hbar}k
\end{aligned}
\end{equation}

With

\begin{equation}\label{eqn:qmSusskind:1120}
\begin{aligned}
\hatk = -i\PD{x}{}
\end{aligned}
\end{equation}

an analogous statement is to write

\begin{equation}\label{eqn:qmSusskind:1140}
\begin{aligned}
\hatp = -i\Hbar \PD{x}{}
\end{aligned}
\end{equation}

Contrast this to 

\begin{equation}\label{eqn:qmSusskind:1160}
\begin{aligned}
\hatx = \delta(x)
\end{aligned}
\end{equation}

Eigenfunctions of these operators are completely incompatible.  No position eigenfunction can be a momentum eigenfunction.  The idea so far was just to provide some intuitive guidance.  The wave function peaked in space corresponds to, roughly, the position of the particle, and the completely spread (will see this to be a Fourier transform later) wavefunction corresponds to the momentum of the particle.

\subsection{Proper characterization as momentum}

There is a discussion in the lectures mentioning that a wave function that is peaked at a position in space
corresponds to the position of a particle in an approximate, but intuitively reasonable seeming fashion.

It is mentioned however, to properly show the same sort of characterization for the 
equivalence of the QM momentum construction and momentum of classical
physics requires the consideration of a (large and massive) wave packet.  It can be shown that the equations that
govern the motion
of such an object approximate familiar Newtonian dynamics in this limit.  It will be interesting to see how
this pans out.  My assumption is that if we start from the relativistic (Dirac) wave equations, we can also
get the relativistic dynamics equations, and presumably also ideas like the classical current density vector
of electromagnetism.  Very exciting to see that it is at least possible to formulate the classical results from
more fundamental underlying principles, even if I do not know what those are yet.  How will all of this relate 
to the Lagrangian formulation that we can express Newtonian and relativistic dynamics and electromagnetism using.
Can one in fact produce the classical Lagrangians for (proper) Lorentz force, and Maxwell's equation (or at least the \(A \cdot J\) term) directly from the QM Lagrangians?

\section{Lecture 4}

Position and momentum treatment of wavefunctions on a circle. 

\section{Lecture 5}

Fourier transform is expressed as an inner product relating momentum and 
position eigenvectors basis using the identity transformation

\begin{equation}\label{eqn:qmSusskind:1180}
\begin{aligned}
I = \sum_n \ketbra{n}{n}
\end{aligned}
\end{equation}

using exponentials and delta functions as the two sets of basis vectors.  Very
slick!  Write this up.

%\bibliographystyle{plainnat}
%\bibliography{myrefs}

%\end{document}
