%
% Copyright © 2013 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\makeoproblem{}{problem:desaiDiracNotesProblems:1}{\citep{desai2009quantum} pr 1.1}{
FIXME: description?
} % problem

\makeanswer{problem:desaiDiracNotesProblems:1}{
With
\begin{equation}\label{eqn:desaiDiracNotesProblems:95}
\begin{aligned}
\ket{\alpha_1} &\equiv 
\begin{bmatrix}
1 \\
0
\end{bmatrix} \\
\ket{\alpha_2} &\equiv 
\begin{bmatrix}
0 \\
1
\end{bmatrix} \\
\bra{\alpha_1} &\equiv 
\begin{bmatrix}
1 & 0
\end{bmatrix} \\
\bra{\alpha_2} &\equiv 
\begin{bmatrix}
0 & 1
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{i. Orthonormal}

Straight multiplication is sufficient to show this and we get

\begin{equation}\label{eqn:desaiDiracNotesProblems:115}
\begin{aligned}
\braket{\alpha_1}{\alpha_1} &= 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
\begin{bmatrix}
1 \\
0
\end{bmatrix} =
\begin{bmatrix}
1
\end{bmatrix} \\
\braket{\alpha_2}{\alpha_2} &= 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
\begin{bmatrix}
0 \\
1
\end{bmatrix} =
\begin{bmatrix}
1
\end{bmatrix} \\
\braket{\alpha_1}{\alpha_2} &= 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
\begin{bmatrix}
0 \\
1
\end{bmatrix} =
\begin{bmatrix}
0
\end{bmatrix} \\
\braket{\alpha_2}{\alpha_1} &= 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
\begin{bmatrix}
1 \\
0
\end{bmatrix} =
\begin{bmatrix}
0
\end{bmatrix} \\
\end{aligned}
\end{equation}

\paragraph{ii. Linear combinations for state vectors}

\begin{equation}\label{eqn:desaiDiracNotesProblems:135}
\begin{bmatrix}
a \\
b
\end{bmatrix} = 
a \ket{\alpha_1}
+b \ket{\alpha_2}
\end{equation}

\paragraph{iii. Outer products}

We have

\begin{equation}\label{eqn:desaiDiracNotesProblems:155}
\begin{aligned}
\ket{\alpha_1}\bra{\alpha_2} &=
\begin{bmatrix}
1 \\
0
\end{bmatrix} 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
=
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix} \\
\ket{\alpha_2}\bra{\alpha_1} &=
\begin{bmatrix}
0 \\
1
\end{bmatrix} 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
=
\begin{bmatrix}
0 & 0 \\
1 & 0
\end{bmatrix} \\
\ket{\alpha_1}\bra{\alpha_1} &=
\begin{bmatrix}
1 \\
0
\end{bmatrix} 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
=
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} \\
\ket{\alpha_2}\bra{\alpha_2} &=
\begin{bmatrix}
0 \\
1
\end{bmatrix} 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
=
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} \\
\end{aligned}
\end{equation}

\paragraph{iv. Completeness relation}

From the above outer products, summation over just the diagonal terms we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:175}
\ket{\alpha_1}\bra{\alpha_1} + \ket{\alpha_2}\bra{\alpha_2} =
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} +
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} 
= I
\end{equation}

\paragraph{v. Arbitrary matrix as sum of outer products}

By inspection

\begin{equation}\label{eqn:desaiDiracNotesProblems:195}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} 
=
a \ket{\alpha_1}\bra{\alpha_1} 
+b \ket{\alpha_1}\bra{\alpha_2} 
+c \ket{\alpha_2}\bra{\alpha_1} 
+d \ket{\alpha_2}\bra{\alpha_2}
\end{equation}

\paragraph{vi. Spin matrix}

Given

\begin{equation}\label{eqn:desaiDiracNotesProblems:215}
\begin{aligned}
A \ket{\alpha_1} &= + \ket{\alpha_1} \\
A \ket{\alpha_2} &= - \ket{\alpha_1}
\end{aligned}
\end{equation}

Our matrix elements are
\begin{equation}\label{eqn:desaiDiracNotesProblems:235}
\begin{aligned}
\bra{\alpha_1} A \ket{\alpha_1} &= 1 \\
\bra{\alpha_2} A \ket{\alpha_1} &= 0 \\
\bra{\alpha_1} A \ket{\alpha_2} &= 0 \\
\bra{\alpha_2} A \ket{\alpha_2} &= -1
\end{aligned}
\end{equation}

Thus the matrix representation of the operator \(A\) with respect to basis \(\{\alpha_1, \alpha_2\}\) is

\begin{equation}\label{eqn:desaiDiracNotesProblems:255}
\{A\} =
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix} 
\end{equation}

} % answer

\makeoproblem{Derivative of inverse operator}{problem:desaiDiracNotesProblems:2}{\citep{desai2009quantum} pr 1.2}{
FIXME: describe.
} % problem

\makeanswer{problem:desaiDiracNotesProblems:2}{
We take derivatives of the identity operator, giving

\begin{equation}\label{eqn:desaiDiracNotesProblems:275}
\begin{aligned}
0 
&= \frac{dI}{d\lambda} \\
&= \frac{d (A A^{-1})}{d\lambda} \\
&= \frac{d A }{d\lambda} A^{-1} + A \frac{d A^{-1}}{d\lambda} \\
\end{aligned}
\end{equation}

left multiplication by \(A^{-1}\) and rearranging we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:295}
\frac{d A^{-1}}{d\lambda} 
= -A^{-1} \frac{d A }{d\lambda} A^{-1} 
\end{equation}

as desired.

} % answer

\makeoproblem{Unitary representations}{problem:desaiDiracNotesProblems:3}{\citep{desai2009quantum} pr 1.3}{
Show that a unitary operator \(U\) can be written

\begin{equation}\label{eqn:desaiDiracNotesProblems:315}
U = \frac{1 + iK}{1-iK},
\end{equation}

where \(K\) is a Hermitian operator.
} % problem

\makeanswer{problem:desaiDiracNotesProblems:3}{

\paragraph{A commutation assumption for the numerator and denominator}

Before tackling the problem, note that with the fraction written this way, and not as

\begin{equation}\label{eqn:desaiDiracNotesProblems:2}
U = (1 + iK)\frac{1}{1-iK},
\end{equation}

or
\begin{equation}\label{eqn:desaiDiracNotesProblems:335}
U = \frac{1}{1-iK}(1 + iK),
\end{equation}

there appears to be an implicit assumption that the numerator and denominator commute.  How can that be justified?

Suppose that the denominator can be expanded in Taylor series

\begin{equation}\label{eqn:desaiDiracNotesProblems:355}
\frac{1}{1-iK} = 1 + iK + (iK)^2 + (iK)^3 + \cdots
\end{equation}

If this converges, this series does in fact commute with the numerator since both are polynomials in \(K\).  Another way of looking at this would be to apply a spectral decomposition to the operators (assumed to be matrices now) where using \(K = V \Sigma V^\dagger\) for a unitary \(V\) and diagonal \(\Sigma\), we can write

\begin{equation}\label{eqn:desaiDiracNotesProblems:375}
U = \frac{1}{1-iK}(1 + iK) = \frac{1}{1-i \Sigma}(1 + i \Sigma)
\end{equation}

Both the numerator and denominator are now diagonal and thus commute.  Generalizing either of these commutation justifications to infinite dimensional Hilbert operators or where that inverse power series in \(K\) does not converge would take further thought.

\paragraph{That this representation is unitary}

From \eqnref{eqn:desaiDiracNotesProblems:2} we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:395}
\begin{aligned}
U U^\dagger 
&= (1 + iK)\frac{1}{1-iK} \frac{1}{1+iK} (1 - iK) \\
&= \frac{1 + K^2}{1+K^2} \\
&= 1
\end{aligned}
\end{equation}

So this operator is unitary for all Hermitian \(K\).  However, is there a \(K\) for any unitary \(U\) that is Hermitian and for which this identity holds true?  We can rearrange for \(K\) to get

\begin{equation}\label{eqn:desaiDiracNotesProblems:3}
K = i \frac{ U - 1 }{U + 1}
\end{equation}

Is this Hermitian?  If so then \(K - K^\dagger = 0\), so let us evaluate that.

\begin{equation}\label{eqn:desaiDiracNotesProblems:415}
K - K^\dagger = i \frac{ U - 1 }{U + 1} + i \frac{ U^\dagger - 1 }{U^\dagger + 1}
\end{equation}

Multiplying by \(-i(U+1)(U^\dagger+1)\) we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:435}
\begin{aligned}
-i(U+1)(U^\dagger+1)(K - K^\dagger) 
&= ( U - 1 )(U^\dagger + 1) + ( U^\dagger - 1 )(U + 1) \\
&= U U^\dagger - 1 - U^\dagger + U     + U^\dagger U - U + U^\dagger - 1 \\
&= 0.
\end{aligned}
\end{equation}

Therefore, provided \(2 + U + U^\dagger \ne 0\) (if it does we only showed that \(0 = 0\)), the operator \(K\) is Hermitian.  The expression \eqnref{eqn:desaiDiracNotesProblems:3} then allows any unitary operator to be expressed as the fraction \eqnref{eqn:desaiDiracNotesProblems:2}.

\paragraph{An exponential representation}

Show that one can also write

\begin{equation}\label{eqn:desaiDiracNotesProblems:455}
U = e^{i C},
\end{equation}

where \(C\) is Hermitian.  Utilizing the power series we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:475}
\begin{aligned}
(e^{iC})^\dagger
&= 
\sum_{k=0}^\infty \inv{k!} ((iC)^k)^\dagger \\
&= 
\sum_{k=0}^\infty \inv{k!} ((-iC)^k) \\
&= 
e^{-iC}.
\end{aligned}
\end{equation}

The operators \(i C\) and \(-i C\) commute, so we can write

\begin{equation}\label{eqn:desaiDiracNotesProblems:495}
(e^{iC})^\dagger e^{iC} = e^{ -iC + iC } = 1,
\end{equation}

which shows that this exponential construction is in fact unitary for any Hermitian \(C\).  The remainder of the exercise requires a demonstration that we can find such an operator \(C\) for any given unitary operator \(U\).  Rearranging, we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:10}
C = -i \ln ( U ).
\end{equation}

How can we give this some meaning?  One way, with the presumption that working with the matrix representation of the operator is allowable, is to utilize the spectral theorem for normal matrices.  Normal here means that the matrix and its Hermitian conjugate commute, which is implied by \(U U^\dagger = 1 = U^\dagger U\).  So we can write, for a diagonal matrix \(\Sigma\), and a unitary matrix \(V\), 
\begin{equation}\label{eqn:desaiDiracNotesProblems:515}
U = V \Sigma V^\dagger,
\end{equation}

so the logarithm of \eqnref{eqn:desaiDiracNotesProblems:10} can be reduced, and we are left with

\begin{equation}\label{eqn:desaiDiracNotesProblems:11}
C = -i V \ln ( \Sigma ) V^\dagger.
\end{equation}

Here the logarithm of the diagonal matrix is nothing more than the diagonal matrix of the eigenvalues.

We still have to show that \(C\) as defined in \eqnref{eqn:desaiDiracNotesProblems:11} is Hermitian.  At a glance it looks like this may be anti Hermitian \(C^\dagger = -C\), but we really need a characterization of the eigenvalues to say.  That conjugate is

\begin{equation}\label{eqn:desaiDiracNotesProblems:11b}
C^\dagger = i V (\ln ( \Sigma ))^\dagger V^\dagger.
\end{equation}

It seems worthwhile to work an example to see if we are even on the right track.  Let us pick the 2 dimensional rotation matrix, and express it using its eigenvalue decomposition.  That is

\begin{equation}\label{eqn:desaiDiracNotesProblems:12}
U = 
\begin{bmatrix}
\cos\theta & \sin\theta \\
-\sin\theta & \cos\theta
\end{bmatrix},
\end{equation}

with decomposition

\begin{equation}\label{eqn:desaiDiracNotesProblems:12b}
\begin{aligned}
V &= \inv{\sqrt{2}} 
\begin{bmatrix}
1 & 1 \\
i & -i \\
\end{bmatrix} \\
\Sigma &= 
\begin{bmatrix}
e^{i\theta} & 0 \\
0 & e^{-i\theta} 
\end{bmatrix} \\
U &= V \Sigma V^\dagger.
\end{aligned}
\end{equation}

We have

\begin{equation}\label{eqn:desaiDiracNotesProblems:535}
\ln \Sigma =
\begin{bmatrix}
i\theta & 0 \\
0 & -i\theta 
\end{bmatrix}.
\end{equation}

Ah.  This is purely imaginary, and accounts for the Hermiticity of \(C\) in this specific example.  Are the logs of the eigenvalues of unitary matrices all purely imaginary?  That seems like a lot to ask for.

Incidentally, for this example, \(C = -i V \ln \Sigma V^\dagger\) gives us

\begin{equation}\label{eqn:desaiDiracNotesProblems:555}
\begin{aligned}
C &= i \theta 
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix} \\
&= \theta \sigma_2,
\end{aligned}
\end{equation}

So, in a rather neat way, we have an matrix exponential expression for the standard planar rotation matrix, in terms of one of the Pauli matrices.  It is straight forward to verify that 

\begin{equation}\label{eqn:desaiDiracNotesProblems:13}
U = e^{i \sigma_2 \theta},
\end{equation}

does in fact recover \eqnref{eqn:desaiDiracNotesProblems:12}.  This follows directly from \((i \sigma_2)^2 = -I\), allowing us to write

\begin{equation}\label{eqn:desaiDiracNotesProblems:14}
U = \cos\theta I + i \sigma_2 \sin\theta.
\end{equation}

Okay.  With that example worked out, we come to the conclusion that the operator specified in \eqnref{eqn:desaiDiracNotesProblems:11}, can be Hermitian.

Having worked an example, we are left to prove the more general case.  To do this we have only to note that the eigenvalues of a Unitary matrix have unit norm, so they must all be of the form \(e^{i\alpha}\).  Suppose we write for the diagonal matrix 

\begin{equation}\label{eqn:desaiDiracNotesProblems:11c}
\Sigma = 
{\begin{bmatrix}
e^{i\alpha_k} \delta_{kj}
\end{bmatrix}}_k.
\end{equation}

The logarithm and its conjugate are then

\begin{equation}\label{eqn:desaiDiracNotesProblems:11d}
\begin{aligned}
\ln \Sigma 
&= 
{\begin{bmatrix}
i\alpha_k \delta_{kj}
\end{bmatrix}}_k \\
(\ln \Sigma)^\dagger &= -\ln \Sigma.
\end{aligned}
\end{equation}

This completes the required proof, showing that the matrix \(C\) is Hermitian

\begin{equation}\label{eqn:desaiDiracNotesProblems:11e}
C = -i V \ln ( \Sigma ) V^\dagger = C^\dagger.
\end{equation}

I initially relied on wikipedia \citep{wiki:unitary} for the hint that Unitary matrices have unit norm eigenvalues (and the wiki article references Shankar, which I do not have).  However, this is straightforward to show.  Suppose that \(x\) is an eigenvector for \(U\) with eigenvalue \(\lambda\), then we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:575}
\begin{aligned}
\braket{U x}{U x}
&= \braket{U^\dagger U x}{x} \\
&= \braket{x}{x},
\end{aligned}
\end{equation}

but we also have

\begin{equation}\label{eqn:desaiDiracNotesProblems:595}
\begin{aligned}
\braket{U x}{U x}
&= \braket{\lambda x}{ \lambda x} \\
&= \Abs{\lambda}^2 \braket{x}{x}.
\end{aligned}
\end{equation}

We must then have \(\Abs{\lambda}^2 = 1\), or \(\lambda = e^{i\alpha}\) for some real \(\alpha\).

\paragraph{Commuting real and imaginary parts}

If 
\begin{equation}\label{eqn:desaiDiracNotesProblems:4}
U = A + iB,
\end{equation}

show that \(A\) and \(B\) commute.

We can form the matrices \(A\), and \(B\) with the usual real and imaginary decomposition, but using Hermitian conjugation.  That is
\begin{equation}\label{eqn:desaiDiracNotesProblems:5}
\begin{aligned}
A &=
\inv{2} ( U + U^\dagger ) \\
B &=
\inv{2i} ( U - U^\dagger ).
\end{aligned}
\end{equation}

Then the commutation question essentially just requires that we show the commutator is zero

\begin{equation}\label{eqn:desaiDiracNotesProblems:615}
\begin{aligned}
A B - B A 
&=
\inv{4i}\left( 
(U + U^\dagger) (U - U^\dagger)
- (U - U^\dagger) (U + U^\dagger)
\right) \\
&=
\inv{4i}\left( 
U^2 + (U^\dagger)^2 + 1 - 1
- (U^2 + (U^\dagger)^2 - 1 + 1 )
\right) \\
&= 0.\qedmarker
\end{aligned}
\end{equation}

Now, if \(U = e^{iC} = A + iB\), we can expand \(U\) trigonometrically, with the typical power series expansions, and can also write

\begin{equation}\label{eqn:desaiDiracNotesProblems:635}
\begin{aligned}
A &= \cos C \\
B &= \sin C 
\end{aligned}
\end{equation}

We can also use the spectral decomposition of \(U\) and \(C\) above in \eqnref{eqn:desaiDiracNotesProblems:11}, to write

\begin{equation}\label{eqn:desaiDiracNotesProblems:655}
\begin{aligned}
A &= V \cosh(\ln \Sigma) V^\dagger \\
B &= -V \sinh(\ln \Sigma) V^\dagger,
\end{aligned}
\end{equation}

and again here the functions of matrices are nothing more than diagonal evaluation of the respective functions to each of the eigenvalues of \(\Sigma\).
} % answer

\makeoproblem{Determinant of exponential in terms of trace.}{problem:desaiDiracNotesProblems:4}{\citep{desai2009quantum} pr 1.4}{
Show 

\begin{equation}\label{eqn:desaiDiracNotesProblems:675}
\det (e^A) = e^{\tr A}.
\end{equation}

The problem does not put constraints (ie: no statement that \(A\) is Hermitian), so we can not assume a Unitary diagonalization is possible.  We can however assume an upper triangular similarity transformation of the form

\begin{equation}\label{eqn:desaiDiracNotesProblems:695}
A = W J W^{-1},
\end{equation}

where \(W\) is invertible, but not necessarily unitary, and \(J\) is in Jordan Canonical form.  That form is upper triangular with the eigenvalues on the diagonal, and only ones or zeros above the diagonal (however, for the purposes of this problem we only need to know that it is upper triangular).

} % problem

\makeanswer{problem:desaiDiracNotesProblems:4}{
The determinant of \(e^A\) is then

\begin{equation}\label{eqn:desaiDiracNotesProblems:715}
\begin{aligned}
\det(e^A) 
&=
\det(W) \det(e^J) \det(W^{-1}) \\
&=
\det(e^J).
\end{aligned}
\end{equation}

Note that the exponential of a triangular matrix has the exponentials of the eigenvalues along the diagonal.  We can see this by computing the square of an upper triangular matrix in block form.  A general proof of this is straightforward, but one gets the idea by considering the two by two case

\begin{equation}\label{eqn:desaiDiracNotesProblems:735}
\begin{bmatrix}
a & c \\
0 & b
\end{bmatrix}
\begin{bmatrix}
a & c \\
0 & b
\end{bmatrix}
=
\begin{bmatrix}
a^2 & (a + b)c \\
0 & b^2
\end{bmatrix}.
\end{equation}

Forming the exponential series, one is left with exponentials of the eigenvalues along the diagonal.  So we have for our determinant

\begin{equation}\label{eqn:desaiDiracNotesProblems:755}
\begin{aligned}
\det(e^A) 
&=
\det(e^J) \\
&=
\prod_k e^{\lambda_k} \\
&=
e^{\sum_k \lambda_k} \\
&=
e^{\tr(A)}. \qedmarker
\end{aligned}
\end{equation}

} % answer

\makeoproblem{Trace of an outer product operator}{problem:desaiDiracNotesProblems:5}{\citep{desai2009quantum} pr 1.5}{

Show that 

\begin{equation}\label{eqn:desaiDiracNotesProblems:775}
\tr( \ket{\alpha}\bra{\beta} ) = \braket{\beta}{\alpha}.
\end{equation}

} % problem

\makeanswer{problem:desaiDiracNotesProblems:5}{
Let \(A = \ket{\alpha}\bra{\beta}\), and introduce a complete basis \(\ket{e_k}\).  The trace with respect to this basis (or any) is thus

\begin{equation}\label{eqn:desaiDiracNotesProblems:795}
\begin{aligned}
\tr(A) 
&= \sum_k \bra{e_k} A \ket{e_k} \\
&= \sum_k \bra{e_k} \left( \ket{\alpha} \bra{\beta} \right) \ket{e_k} \\
&= \sum_k \braket{\beta} {e_k} \braket{e_k} {\alpha} \\
&= \bra{\beta} \left( \sum_k \ket{e_k} \bra{e_k} \right) \ket{\alpha} \\
&= \bra{\beta} I \ket{\alpha} \\
&= \braket{\beta} {\alpha}. \qedmarker
\end{aligned}
\end{equation}
} % answer

\makeoproblem{eigen calculation}{problem:desaiDiracNotesProblems:6}{\citep{desai2009quantum} pr 1.6}{
For operator(s)

\begin{equation}\label{eqn:desaiDiracNotesProblems:60}
A = 
\ket{\alpha}\bra{\alpha}
+ \lambda \ket{\beta}\bra{\alpha}
+ \lambda^\conj \ket{\alpha}\bra{\beta}
\pm \ket{\beta}\bra{\beta},
\end{equation}

where \(\braket{\alpha}{\beta} = 0\), and \(\braket{\alpha}{\alpha} = \braket{\beta}{\beta} = 1\), find the eigenvalues and vectors for (i) \(\lambda = 1\), and (ii) \(\lambda = i\).
} % problem

\makeanswer{problem:desaiDiracNotesProblems:6}{

\paragraph{Without using matrix representation}
Our eigenvector must be some linear combination of the two kets, so lets look for one of the form \(\ket{e} = \ket{\alpha} + a\ket{\beta}\), and use this to find eigenvalues for

\begin{equation}\label{eqn:desaiDiracNotesProblems:61}
A \ket{e} = b \ket{e}.
\end{equation}

This means we seek solutions to

\begin{equation}\label{eqn:desaiDiracNotesProblems:62}
\ket{\alpha}
+ \lambda \ket{\beta}
+ a \lambda^\conj \ket{\alpha}
\pm a \ket{\beta}
= b ( \ket{\alpha} + a \ket{\beta} ).
\end{equation}

This supplies a pair of simultaneous equations
\begin{equation}\label{eqn:desaiDiracNotesProblems:63}
\begin{aligned}
1 + a \lambda^\conj &= b \\
\lambda \pm a &= b a.
\end{aligned}
\end{equation}

We have our eigenvalue \(b\) in terms of the constant \(a\) immediately, so for \(a\) we wish to solve the quadratic

\begin{equation}\label{eqn:desaiDiracNotesProblems:64}
\lambda \pm a = (1 + a \lambda^\conj ) a
\end{equation}

Let us treat these four cases separately, starting the two \(\lambda = 1\) operators.  Those quadratics are

\begin{equation}\label{eqn:desaiDiracNotesProblems:65}
\begin{aligned}
1 + a &= (1 + a ) a \\
1 - a &= (1 + a ) a
\end{aligned}
\end{equation}

with respective solutions

\begin{equation}\label{eqn:desaiDiracNotesProblems:66}
\begin{aligned}
a &= \pm 1 \\
a &= \pm \sqrt{2} - 1
\end{aligned}
\end{equation}

Summarizing the operator, eigenvalue, and eigenvector triplets for this \(\lambda = 1\) case we have

\begin{subequations}
\label{eqn:desaiDiracNotesProblems:67}
\begin{equation}\label{eqn:desaiDiracNotesProblems:815}
\begin{aligned}
A &=
\ket{\alpha}\bra{\alpha}
+ \ket{\beta}\bra{\alpha}
+ \ket{\alpha}\bra{\beta}
+ \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} \pm \ket{\beta} \\
\lambda_{\pm} &= 1 \pm 1 
\end{aligned}
\end{equation}
\end{subequations}

and

\begin{subequations}
\label{eqn:desaiDiracNotesProblems:68}
\begin{equation}\label{eqn:desaiDiracNotesProblems:835}
\begin{aligned}
A &=
\ket{\alpha}\bra{\alpha}
+ \ket{\beta}\bra{\alpha}
+ \ket{\alpha}\bra{\beta}
- \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} + (\pm \sqrt{2} - 1) \ket{\beta} \\
\lambda_{\pm} &= \pm \sqrt{2}
\end{aligned}
\end{equation}
\end{subequations}

Now for the pair of \(\lambda = i\) operators, our quadratic is

\begin{equation}\label{eqn:desaiDiracNotesProblems:64i}
i \pm a = (1 - i a) a,
\end{equation}

or separately
\begin{equation}\label{eqn:desaiDiracNotesProblems:64ii}
\begin{aligned}
a^2 + 1 &= 0 \\
%a^2 + 2 a i + 1 &= 0 
(a + i)^2 + 2 &= 0
\end{aligned}
\end{equation}

The respective solutions are

\begin{equation}\label{eqn:desaiDiracNotesProblems:66i}
\begin{aligned}
a &= \pm i \\
a &= i (-1 \pm \sqrt{2} ),
\end{aligned}
\end{equation}

with eigenvalues \(b = 1 - i a\), which are respectively

\begin{equation}\label{eqn:desaiDiracNotesProblems:66j}
\begin{aligned}
b &= 1 \pm 1 \\
b &= \pm \sqrt{2}.
\end{aligned}
\end{equation}

Summarizing the results, we have

\begin{subequations}
\label{eqn:desaiDiracNotesProblems:67ip}
\begin{equation}\label{eqn:desaiDiracNotesProblems:855}
\begin{aligned}
A &=
\ket{\alpha}\bra{\alpha}
+ i \ket{\beta}\bra{\alpha}
- i \ket{\alpha}\bra{\beta}
+ \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} \pm i \ket{\beta} \\
\lambda_{\pm} &= 2, 0
\end{aligned}
\end{equation}
\end{subequations}

and
\begin{subequations}
\label{eqn:desaiDiracNotesProblems:67in}
\begin{equation}\label{eqn:desaiDiracNotesProblems:875}
\begin{aligned}
A &=
\ket{\alpha}\bra{\alpha}
+ i \ket{\beta}\bra{\alpha}
- i \ket{\alpha}\bra{\beta}
+ \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} + i (-1 \pm \sqrt{2} ) \ket{\beta} \\
\lambda_{\pm} &= \pm \sqrt{2}
\end{aligned}
\end{equation}
\end{subequations}

So it appears we got the same eigenvalues and vectors for both \(\lambda = 1\) and \(\lambda = i\).  Is there a higher order principle that this follows from?  Perhaps the fact that both terms with \(\lambda\) coefficients were conjugate pairs?  That is something perhaps worth thinking about.

\paragraph{Using matrix representation}

In the matrix notation with basis \(\{\sigma_1, \sigma_2\} = \{(1,0), (0,1)\}\), and \(A_{mn} = \bra{\sigma_m} A \ket{\sigma_n}\), we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:895}
\begin{aligned}%\label{eqn:desaiDiracNotesProblems:70}
A_{11} &= \bra{\sigma_1} A \ket{\sigma_1} = \braket{\alpha}{\alpha} = 1 \\
A_{22} &= \bra{\sigma_2} A \ket{\sigma_2} = \mu \braket{\beta}{\beta} = \mu \\
A_{12} &= \bra{\sigma_1} A \ket{\sigma_2} = \lambda^\conj \\
A_{21} &= \bra{\sigma_2} A \ket{\sigma_1} = \lambda
\end{aligned}
\end{equation}

Or in whole matrix notation, we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:71}
\{ A \} = 
\begin{bmatrix}
1 & \lambda^\conj \\
\lambda & \mu
\end{bmatrix}.
\end{equation}

Finding the eigenvalues and vectors becomes a straightforward, albeit somewhat tedious, algebraic job, solving for \(\Abs{ A - \sigma I } = 0\), for eigenvalues \(\sigma\).  Doing this, I get

\begin{itemize}
\item \(\lambda = 1\), \(\mu = 1\)

\begin{equation}\label{eqn:desaiDiracNotesProblems:72}
\begin{aligned}
\sigma &= 2,0 \\
\ket{\sigma_2} &= \inv{\sqrt{2}} (1,1) \\
\ket{\sigma_0} &= \inv{\sqrt{2}} (1,-1) 
\end{aligned}
\end{equation}

Alternatively, for the \(\sigma=2\) case we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:72i}
\ket{\sigma_2} = \inv{\sqrt{2}}\left( \ket{\alpha} + \ket{\beta} \right),
\end{equation}

and for the \(\sigma=0\) case we have

\begin{equation}\label{eqn:desaiDiracNotesProblems:72ii}
\ket{\sigma_0} = \inv{\sqrt{2}}\left( \ket{\alpha} - \ket{\beta} \right).
\end{equation}

Ignoring the normalization constant used here, this is consistent with \eqnref{eqn:desaiDiracNotesProblems:67} as it should be.

\item \(\lambda = 1\), \(\mu = -1\)

\begin{equation}\label{eqn:desaiDiracNotesProblems:73}
\begin{aligned}
\sigma &= \pm \sqrt{2} \\
\ket{\sigma_{\pm}} &\propto (1,-1 \pm \sqrt{2}) 
\end{aligned}
\end{equation}

Normalization was not bothered with this time due to pesky \(\sqrt{2}\) terms.  The eigenstates expressed in terms of the original basis vectors are 

\begin{equation}\label{eqn:desaiDiracNotesProblems:73i}
\ket{\sigma_{\pm}} = \ket{\alpha} + (-1 \pm \sqrt{2}) \ket{\beta}
\end{equation}

This is consistent with \eqnref{eqn:desaiDiracNotesProblems:68} as expected.

\item \(\lambda = i\), \(\mu = 1\)

\begin{equation}\label{eqn:desaiDiracNotesProblems:74}
\begin{aligned}
\sigma &= 2,0 \\
\ket{\sigma_0} &= \inv{\sqrt{2}} (i,1) \\
\ket{\sigma_2} &= \inv{\sqrt{2}} (-i,1) 
\end{aligned}
\end{equation}

In terms of the original basis vectors this is 
\begin{equation}\label{eqn:desaiDiracNotesProblems:74i}
\ket{\sigma_0} = \inv{\sqrt{2}}\left( i\ket{\alpha} + \ket{\beta} \right)
\end{equation}

\begin{equation}\label{eqn:desaiDiracNotesProblems:74ii}
\ket{\sigma_2} = \inv{\sqrt{2}}\left( -i\ket{\alpha} + \ket{\beta} \right)
\end{equation}

Checking against \eqnref{eqn:desaiDiracNotesProblems:67ip} shows that \(\ket{\sigma_2}\) above only differs by a constant as expected.

\item \(\lambda = i\), \(\mu = -1\)

\begin{equation}\label{eqn:desaiDiracNotesProblems:75}
\begin{aligned}
\sigma &= \pm \sqrt{2} \\
\ket{\sigma_{\pm}} &\propto (1, -i(1 \mp \sqrt{2}))
\end{aligned}
\end{equation}

Or, in terms of the original basis,

\begin{equation}\label{eqn:desaiDiracNotesProblems:75i}
\ket{\sigma_{\pm}} = \ket{\alpha} + i (-1 \pm \sqrt{2}) \ket{\beta}.
\end{equation}

This matches the previous calculation summarized by \eqnref{eqn:desaiDiracNotesProblems:67in}.

\end{itemize}
} % answer
