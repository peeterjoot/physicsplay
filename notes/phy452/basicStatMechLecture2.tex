%
% Copyright © 2013 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%\input{../blogpost.tex}
%\renewcommand{\basename}{basicStatMechLecture2}
%\renewcommand{\dirname}{notes/phy452/}
%\newcommand{\keywords}{Statistical mechanics, Stirling approximation, Probability, Central limit theorem, PHY452H1S}
%\input{../peeter_prologue_print2.tex}
%
%\beginArtNoToc
%\generatetitle{PHY452H1S Basic Statistical Mechanics.  Lecture 2: Probability.  Taught by Prof.\ Arun Paramekanti}
%\chapter{Probability}
\label{chap:basicStatMechLecture2}

%\section{Disclaimer}
%
%Peeter's lecture notes from class.  May not be entirely coherent.

\section{Probability}

The discrete case is plotted roughly in \cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig1}.
%P(x) : F1

\imageFigure{../../figures/phy452/basicStatMechLecture2Fig1}{Discrete probability distribution}{fig:basicStatMechLecture2:basicStatMechLecture2Fig1}{0.3}

\begin{subequations}
\begin{equation}\label{eqn:basicStatMechLecture2:20}
P(x) \ge 1
\end{equation}
\begin{equation}\label{eqn:basicStatMechLecture2:40}
\sum_x P(x) = 1
\end{equation}
\end{subequations}

A continuous probability distribution may look like \cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig2}.

\imageFigure{../../figures/phy452/basicStatMechLecture2Fig2}{Continuous probability distribution}{fig:basicStatMechLecture2:basicStatMechLecture2Fig2}{0.3}

\begin{subequations}
\begin{equation}\label{eqn:basicStatMechLecture2:60}
\mathcal{P}(x) > 0
\end{equation}
\begin{equation}\label{eqn:basicStatMechLecture2:80}
\int \mathcal{P}(x) dx = 1
\end{equation}
\end{subequations}

Probability that event is in the interval \(x_1 - x_0 = \Delta x\) is

\begin{equation}\label{eqn:basicStatMechLecture2:100}
\int_{x_0}^{x_1} \mathcal{P}(x) dx
\end{equation}

\section{Central limit theorem}

\begin{equation}\label{eqn:basicStatMechLecture2:120}
x \leftrightarrow P(x)
\end{equation}

Suppose we construct a sum of random variables

\begin{equation}\label{eqn:basicStatMechLecture2:140}
X = \sum_{i = 1}^N x_i
\end{equation}

\makeexample{Gambling, coin toss}{example:basicStatMechLecture2:1}{

\begin{equation}\label{eqn:basicStatMechLecture2:160}
x \rightarrow 
\left\{
\begin{array}{l l}
+1 & \quad \mbox{Heads} \\
-1 & \quad \mbox{Tails} 
\end{array}
\right.
\end{equation}

If we ask the question about what the total number of heads minus the total number of tails (do we have excess heads, and by how much).
} % makeexample

Given an \textunderline{average} of

\begin{equation}\label{eqn:basicStatMechLecture2:180}
\expectation{x} = \mu
\end{equation}

and a \underlineAndIndex{variance} (or squared standard deviation) of

\begin{equation}\label{eqn:basicStatMechLecture2:200}
\expectation{x^2} - \expectation{x}^2 = \sigma^2
\end{equation}

we have for the sum of random variables

\begin{subequations}
\boxedEquation{eqn:basicStatMechLecture2:220}{
\lim_{N \rightarrow \infty} P(X)
= \inv{\sigma \sqrt{2 \pi N}} \exp\left( - \frac{ (x - N \mu)^2}{2 N \sigma^2} \right)
}
\begin{equation}\label{eqn:basicStatMechLecture2:240}
\expectation{X} = N \mu
\end{equation}
\begin{equation}\label{eqn:basicStatMechLecture2:260}
\expectation{X^2} - \expectation{X}^2 = N \sigma^2
\end{equation}
\end{subequations}

A proof of this can be found in \citep{peterYoungClt}.

\section{Binomial distribution}

\makeexample{Coin toss}{example:basicStatMechLecture2:2}{

Given

\begin{subequations}
\begin{equation}\label{eqn:basicStatMechLecture2:280}
P(\text{Heads}) = \inv{2}
\end{equation}
\begin{equation}\label{eqn:basicStatMechLecture2:300}
P(\text{Tails}) = \inv{2}
\end{equation}
\end{subequations}

Our probability distribution may look like \cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig3}.

\imageFigure{../../figures/phy452/basicStatMechLecture2Fig3}{Discrete probability distribution for Heads and Tails coin tosses}{fig:basicStatMechLecture2:basicStatMechLecture2Fig3}{0.3}

\paragraph{Aside: continuous analogue}

Note that the continuous analogue of a distribution like this is

\begin{equation}\label{eqn:basicStatMechLecture2:320}
\mathcal{P}(x) = \inv{2} \delta(x - 1) + \inv{2} \delta(x + 1)
\end{equation}

\paragraph{2 tosses:}

\begin{equation}\label{eqn:basicStatMechLecture2:340}
(x_1, x_2) 
\in \{
(1, 1), (1, -1), (-1, 1), (-1, -1)
\}
\end{equation}

\begin{equation}\label{eqn:basicStatMechLecture2:360}
X \in \{-2, 0, 0, 2\}
\end{equation}

%\cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig4}.
\imageFigure{../../figures/phy452/basicStatMechLecture2Fig4}{2 tosses distribution}{fig:basicStatMechLecture2:basicStatMechLecture2Fig4}{0.3}

\paragraph{3 tosses}

\begin{equation}\label{eqn:basicStatMechLecture2:380}
(x_1, x_2, x_3) 
\in \{
(1, 1, 1), \cdots (-1, -1, -1)
\}
\end{equation}

\begin{itemize}
\item \(X = 3\) : 1 way
\item \(X = 1\) : 3 ways
\item \(X = -1\) : 3 ways
\item \(X = -3\) : 1 way
\end{itemize}

%\cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig5}.
\imageFigure{../../figures/phy452/basicStatMechLecture2Fig5}{3 tosses}{fig:basicStatMechLecture2:basicStatMechLecture2Fig5}{0.3}

\paragraph{N tosses}

We want to find \(P_N(X)\).  We have

\begin{equation}\label{eqn:basicStatMechLecture2:400}
\text{Total Heads} - \text{Total Tails} = X
\end{equation}
\begin{equation}\label{eqn:basicStatMechLecture2:420}
\text{Total tosses} = \text{Total Tails} + \text{Total Heads} = N
\end{equation}

So that
\begin{itemize}
\item Total Heads: \(\frac{N + X}{2}\)
\item Total Tails: \(\frac{N - X}{2}\)
\end{itemize}

How many ways can we find a specific event such as the number of ways we find \(2\) heads and \(1\) tail?  We can enumerate these \(\{ (H, H, T), (H, T, H), (T, H, H)\}\).

The number of ways of choosing just that combination is

\begin{equation}\label{eqn:basicStatMechLecture2:440}
P_N(X) = 
\left(\inv{2}\right)^N 
\binom{N}{\frac{N-X}{2}} \quad\mbox{or}\quad
\left(\inv{2}\right)^N 
\binom{N}{\frac{N+X}{2}}
\end{equation}

we find the \underlineAndIndex{binomial distribution}

\begin{equation}\label{eqn:basicStatMechLecture2:460}
P_N(X) 
= 
\left\{
\begin{array}{l l}
\left(\inv{2}\right)^N 
\frac{N!}{
\left(\frac{N-X}{2}\right)!
\left(\frac{N+X}{2}\right)!
}
& \quad \mbox{if \(X\) and \(N\) have same parity} \\
0& \quad \mbox{otherwise} 
\end{array}
\right.
\end{equation}

The reason for special casing this depending on the parity is because we'll have only \(X\) even or \(X\) odd, depending on whether \(N\) is even or odd.  For example when \(N = 3\) we have \(X \in \{-3, -1, 1, 3\}\), and for \(N = 2\) we have \(X \in \{-2, 0, 2\}\).  In particular once we have the Gaussian approximation that we are looking for (for large \(N\)), should we use an integral to approximate sum, and check for normalization as in \cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig9}, we'd find a normalization of two instead of unity, because we are integrating over all ``integers'' (approximated by the continuum), instead of just the odd or the even integers.

\imageFigure{../../figures/phy452/basicStatMechLecture2Fig9}{Gaussian approximation of discrete distribution with parity}{fig:basicStatMechLecture2:basicStatMechLecture2Fig9}{0.3}

For the approximation itself, we'll use the Stirling formula \eqnref{eqn:basicStatMechLecture2:480}, to find

%\begin{equation}\label{eqn:basicStatMechLecture2:480}
%N! \approx \sqrt{ 2 \pi N} N^N e^{-N},
%\end{equation}
%
\begin{dmath}\label{eqn:basicStatMechLecture2:500}
P_N(X) 
= 
\left(\inv{2}\right)^N 
\frac{N!}{
\left(\frac{N-X}{2}\right)!
\left(\frac{N+X}{2}\right)!
}
%\approx
%\left( \inv{2} \right)^N 
%\frac
%{ e^{-N} N^N \sqrt{ 2 \pi N} }
%{ 
%e^{-\frac{N+X}{2}} \left( \frac{N+X}{2}\right)^{\frac{N+X}{2}} \sqrt{ 2 \pi \frac{N+X}{2}} 
%e^{-\frac{N-X}{2}} \left( \frac{N-X}{2}\right)^{\frac{N-X}{2}} \sqrt{ 2 \pi \frac{N-X}{2}} 
%}
%=
%\left( \inv{2} \right)^N 
%\frac
%{ 2 N^N \sqrt{ N} }
%{ 
%\sqrt{2 \pi}
%\left( \frac{N+X}{2}\right)^{\frac{N+X}{2}} \sqrt{ N^2 - X^2} 
%\left( \frac{N-X}{2}\right)^{\frac{N-X}{2}} 
%}
%=
%\frac
%{ 2 N^N \sqrt{ N} }
%{ 
%\sqrt{2 \pi}
%\left( N^2 - X^2 \right)^{N/2 + 1/2}
%\left( \frac{N+X}{N-X}\right)^{X/2} 
%}
\end{dmath}

This can be simplified to

\begin{equation}\label{eqn:basicStatMechLecture2:520}
P_N(X) 
= 
\frac{2}{\sqrt{2 \pi N}} \exp\left( -\frac{X^2}{2N} \right).
\end{equation}

Demonstrating this limit is left to \prref{basicStatMech:problemSet1:1}.
} % makeexample

\section{Stirling formula}

To prove the Stirling formula we'll use the Gamma (related) \index{gamma function} function

\begin{equation}\label{eqn:basicStatMechLecture2:540}
I(\alpha) = \Gamma(\alpha + 1) = \int_0^\infty dy e^{-y} y^\alpha
\end{equation}

Observe that we have

\begin{equation}\label{eqn:basicStatMechLecture2:560}
I(0) = \int_0^\infty dy e^{-y} = 1,
\end{equation}

and

\begin{dmath}\label{eqn:basicStatMechLecture2:570}
I(\alpha + 1) 
= \int_0^\infty dy e^{-y} y^{\alpha + 1}
= \int_0^\infty d \left( \frac{e^{-y}}{-1} \right) y^{\alpha + 1}
= 
\cancel{\evalrange{\left( \frac{e^{-y}}{-1} y^{\alpha + 1} \right)}{0}{\infty}}
-
\int_0^\infty dy \left( \frac{e^{-y}}{-1} \right) (\alpha + 1) y^{\alpha}
= (\alpha + 1)I(\alpha).
\end{dmath}

This induction result means that

\begin{equation}\label{eqn:basicStatMechLecture2:600}
I(\alpha = N) = N!,
\end{equation}

so we can use the large \(\alpha\) behavior of this function to find approximations of the factorial.  What does the innards of this integral (integrand) look like.  We can plot these \cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig7}, and find a hump for any non-zero value of \(\alpha\) (\(\alpha = 0\) is just a line)

\imageFigure{../../figures/phy452/basicStatMechLecture2Fig7}{Some values of \(f(y, \alpha)\)}{fig:basicStatMechLecture2:basicStatMechLecture2Fig7}{0.3}

There's a peak for large alpha that can be approximated by a Gaussian function.  When \(\alpha\) is large enough then we can ignore the polynomial boundary effects.  We want to look at where this integrand is peaked.  We can write

\begin{equation}\label{eqn:basicStatMechLecture2:620}
I(\alpha) 
= \int_0^\infty dy e^{-y + \alpha \ln y} 
= \int_0^\infty dy f(y),
\end{equation}

and look for where \(f(y)\) is the largest.  We've set

\begin{equation}\label{eqn:basicStatMechLecture2:640}
f(y) = -y + \alpha \ln y,
\end{equation}

and want to look at where

\begin{dmath}\label{eqn:basicStatMechLecture2:660}
0 
= \evalbar{f'(y)}{y^\conj}
= -1 + \frac{\alpha}{y^\conj}
\end{dmath}

so that the peak value

\begin{equation}\label{eqn:basicStatMechLecture2:680}
y^\conj = \alpha.
\end{equation}

We now want to expand the integrand around this peak value

\begin{equation}\label{eqn:basicStatMechLecture2:700}
I(\alpha) = \int_0^\infty
\exp\left(
f(y^\conj) + 
\mathLabelBox
[
   labelstyle={xshift=2cm, yshift=0.5cm},
   linestyle={out=270,in=90, latex-}
]
{
\evalbar{\PD{y}{f}}{y^\conj}
}{=0}
(y - y^\conj) 
+ 
\inv{2}
\evalbar{\PDSq{y}{f}}{y^\conj}
(y - y^\conj)^2
+ \cdots
\right)
\end{equation}

We'll drop all but the quadratic term, and first need the second derivative

\begin{dmath}\label{eqn:basicStatMechLecture2:780}
f''(y) 
= \frac{d}{dy} \left(-1 + \alpha \inv{y}\right)
= -\alpha \inv{y^2},
\end{dmath}

at \(y = y^\conj = \alpha\) we have \(f''(y^\conj) = -\inv{\alpha}\), so

\begin{dmath}\label{eqn:basicStatMechLecture2:720}
I(\alpha \gg 1) \approx 
e^{f(y^\conj)}
\int_0^\infty
\exp\left(
\inv{2}
\evalbar{\PDSq{y}{f}}{y^\conj}
(y - y^\conj)^2
\right)
=
e^{f(\alpha)}
\int_0^\infty dy 
e^{
-\frac{(y - \alpha)^2}{2 \alpha }
}.
\end{dmath}

With a substitution \(x = (y - \alpha)/\sqrt{2 \alpha}\), this integral can be split into a constant term and an error function component, where the error function is

\begin{equation}\label{eqn:basicStatMechLecture2:820b}
\erf(z) = \frac{2}{\sqrt{\pi}} \int_0^z e^{-t^2} dt.
\end{equation}

That is

\begin{dmath}\label{eqn:basicStatMechLecture2:820c}
\int_0^\infty dy 
e^{
-\frac{(y - \alpha)^2}{2 \alpha }
}
= 
\sqrt{ 2\alpha}
\int_{-\alpha/\sqrt{2\alpha}}^\infty dx e^{-x^2}
= 
\sqrt{ 2\alpha}
\int_{-\sqrt{\alpha/2}}^0 dx e^{-x^2}
+
\sqrt{ 2\alpha}
\int_0^\infty dx e^{-x^2}
= 
\sqrt{ 2\alpha}
\lr{ 
\frac{\sqrt{\pi}}{2} \erf\lr{ \sqrt{\frac{\alpha}{2}} }
+
\frac{\sqrt{\pi}}{2} 
},
\end{dmath}

or (as also verified in \nbref{lecture2Figures.nb})

\begin{equation}\label{eqn:basicStatMechLecture2:820}
\int_0^\infty dy 
e^{
-\frac{(y - \alpha)^2}{2 \alpha}
}
=
\sqrt{\frac{\pi  \alpha }{2}} \left(\erf \left(\sqrt{\frac{\alpha }{2}}\right)+1\right).
\end{equation}

Note that \(\erf(z)\) tends to unity as \(x \rightarrow \infty\), as illustrated in \cref{fig:basicStatMechLecture2:basicStatMechLecture2Fig8}.  So we have for large \(\alpha\)

\imageFigure{../../figures/phy452/basicStatMechLecture2Fig8}{Error function}{fig:basicStatMechLecture2:basicStatMechLecture2Fig8}{0.3}

\begin{equation}\label{eqn:basicStatMechLecture2:840}
\int_0^\infty dy 
e^{
-\frac{(y - \alpha)^2}{2 \alpha}
}
\approx \sqrt{2 \pi \alpha}.
\end{equation}

With

\begin{equation}\label{eqn:basicStatMechLecture2:740}
e^{f(\alpha)} = e^{-\alpha + \alpha \ln \alpha},
\end{equation}

we have for \(\alpha \gg 1\)
\begin{equation}\label{eqn:basicStatMechLecture2:760}
I(\alpha) \approx e^{-\alpha} 
\mathLabelBox
[
   labelstyle={xshift=2cm},
   linestyle={out=270,in=90, latex-}
]
{
e^{\alpha \ln \alpha} 
}
{
\(\alpha^\alpha\)
}
\sqrt{2 \pi \alpha}.
\end{equation}

This gives us the \underlineAndIndex{Stirling approximation}

\boxedEquation{eqn:basicStatMechLecture2:480}{
N! \approx \sqrt{ 2 \pi N} N^N e^{-N}.
}

A plot of the relative difference between the stirling approximation and the factorial \((N! - \sqrt{ 2 \pi N} N^N e^{-N})/N!\) is given in \cref{fig:stirlingError:stirlingErrorFig3}

\imageFigure{../../figures/phy452/stirlingErrorFig3}{Relative error of the Stirling approximation}{fig:stirlingError:stirlingErrorFig3}{0.3}

%0.077863, 0.0404978, 0.0272984, 0.020576, 0.0165069, 0.0137803, 0.0118262, 0.0103573, 0.00921276, 0.00829596, 0.00754507, 0.00691879

%\EndArticle
%\EndNoBibArticle
