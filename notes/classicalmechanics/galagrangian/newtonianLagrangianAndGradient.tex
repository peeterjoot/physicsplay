%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Newton's Law from Lagrangian}
\index{Newton's law}
\label{chap:newtonianLagrangianAndGradient}
%\date{August 9, 2008.  newtonianLagrangianAndGradient.tex}

In the classical limit the Lagrangian action for a point particle in a general
position dependent field is:

\begin{equation}
S = \inv{2} m\Bv^2 - \varphi
\end{equation}

Given the Lagrange equations that minimize the action, it is fairly simple
to derive the Newtonian force law.

\begin{equation}\label{eqn:newtonianLagrangianAndGradient:20}
\begin{aligned}
0
&= \PDb{S}{x^i} - \frac{d}{dt}\PDb{S}{\dot{x}^i} \\
&= -\PDb{\varphi}{x^i} - \frac{d}{dt}\left(m \dot{x}^i\right)
\end{aligned}
\end{equation}

Multiplication of this result with the unit vector \(\Be_i\), and summing over
all unit vectors we have:

\begin{equation*}
\sum \Be_i \frac{d}{dt}\left(m \dot{x}^i\right) = - \sum \Be_i \PDb{\varphi}{x^i}
\end{equation*}

Or, using the gradient operator, and writing \(\Bv = \sum \Be_i \dot{x}^i\), we have:

\begin{equation}
\BF = \frac{d (m \Bv)}{dt} = - \grad \varphi
\end{equation}

\section{The mistake hiding above}

Now, despite the use of upper and lower pairs of indices for the basis vectors and coordinates, this
result is not valid for a general set of basis vectors.  This initially confused the author, since the RHS
sum \(\Bv = \sum \Be_i v^i\) is valid for any set of basis vectors independent of the orthonormality of that
set of basis vectors.  This is assuming that these coordinate pairs follow the usual reciprocal relationships:

\begin{equation*}
\Bx = \sum \Be_i x^i
\end{equation*}
\begin{equation*}
x^i = \Bx \cdot \Be^i
\end{equation*}
\begin{equation*}
\Be^i \cdot \Be_j = {\delta^i}_j
\end{equation*}

However, the LHS that implicitly defines the gradient as:
\begin{equation*}
\grad = \sum \Be_i \PDb{}{x^i}
\end{equation*}

is a result that is only valid when the set of basis vectors \(\Be_i\) is orthonormal.  The general result is
expected instead to be:

\begin{equation*}
\grad = \sum \Be^i \PDb{}{x^i}
\end{equation*}

This is how the gradient is defined (without motivation) in Doran/Lasenby.  One can however demonstrate that this definition, and not \(\grad = \sum \Be_i \PDb{}{x^i}\), is required by doing a computation of something like \(\grad \norm{\Bx}^\alpha\) with \(\Bx = \sum x^i \Be_i\) for a general basis \(\Be_i\) to demonstrate this.  An example of this can be found in the appendix below.

So where did things go wrong?  It was in one of the ``obvious'' skipped steps: \(\Bv = \sum \dot{x^i} \dot{x^i}\).  It is in that
spot where there is a hidden orthonormal frame vector requirement since a general basis will have mixed product terms too
(ie: non-diagonal metric tensor).

Expressed in full for general frame vectors the action to minimize is the following:

\begin{equation}
S = \inv{2} m \sum \dot{x}^i \dot{x}^j \Be_i \cdot \Be_j -\varphi
\end{equation}

Or, expressed using a metric tensor \(g_{ij} = \Be_i \cdot \Be_j\), this is:

\begin{equation}
S = \inv{2} m \sum \dot{x}^i \dot{x}^j g_{ij} -\varphi
\end{equation}

\section{Equations of motion for vectors in a general frame}

Now we are in shape to properly calculate the equations of motion from the Lagrangian action minimization equations.

\begin{equation}\label{eqn:newtonianLagrangianAndGradient:40}
\begin{aligned}
0
&= \PDb{S}{x^k} - \frac{d}{dt}\PDb{S}{\dot{x}^k} \\
&= -\PDb{\varphi}{x^k} - \frac{d}{dt} \left( \inv{2} m \sum g_{ij} \PDb{\dot{x}^i}{\dot{x}^k} \dot{x}^j + \dot{x}^i \PDb{\dot{x}^j}{\dot{x}^k} \right) \\
&= -\PDb{\varphi}{x^k} - \frac{d}{dt} \left( \inv{2} m \sum g_{ij} ({\delta^i}_k \dot{x}^j + \dot{x}^i {\delta^j}_k) \right) \\
&= -\PDb{\varphi}{x^k} - \frac{d}{dt} \left( \inv{2} m \sum (g_{kj} \dot{x}^j + g_{ik}\dot{x}^i) \right) \\
&= -\PDb{\varphi}{x^k} - \frac{d}{dt} \left( m \sum g_{kj} \dot{x}^j \right) \\
\frac{d}{dt} \left( m \sum g_{kj} \dot{x}^j \right) &= -\PDb{\varphi}{x^k} \\
\sum \Be^k \frac{d}{dt} \left( m \sum g_{kj} \dot{x}^j \right) &= - \sum \Be^k \PDb{\varphi}{x^k} \\
\frac{d}{dt}
\Bigl( m \sum_j \dot{x}^j
\mathLabelBox{\sum_k \Be^k \Be_k \cdot \Be_j}{\(=\Be_j\)}
\Bigr) &= \\
\frac{d}{dt} \left( m \sum_j \dot{x}^j \Be_j \right) &= \\
\end{aligned}
\end{equation}

The requirement for reciprocal pairs of coordinates and basis frame vectors is due to the
summation \(\Bv = \sum \Be_i \dot{x}^i\), and it allows us to write all of the Lagrangian equations
in vector form for an arbitrary frame basis as:

\begin{equation}
\BF = \frac{d (m \Bv)}{dt} = - \sum \Be^k \PDb{\varphi}{x^k}
\end{equation}

If we are calling this RHS a gradient relationship in an orthonormal frame, we therefore must define
the following as the gradient for the general frame:

\begin{equation}\label{eqn:newtonLaG:gradient}
\grad = \sum \Be^k \PDb{}{x^k}
\end{equation}

The Lagrange equations that minimize the action still generate equations of
motion that hold when the coordinate and basis vectors cannot be summed in this fashion.
In such a case, however, the ability to merge the generalized coordinate equations of motion into a single
vector relationship will not be possible.

\section{Appendix.  Scratch calculations}

\section{frame vector in terms of metric tensor, and reciprocal pairs}

\begin{equation}\label{eqn:newtonianLagrangianAndGradient:60}
\begin{aligned}
e_j &= \sum a_k e^k \\
e_j \cdot e_k &= \sum a_i e^i \cdot e_k \\
e_j \cdot e_k &= a_k \\
\implies & \\
e_j &= \sum e_j \cdot e_k e^k \\
e_j &= \sum g_{jk} e^k
\end{aligned}
\end{equation}

\section{Gradient calculation for an absolute vector magnitude function}

As a verification that the gradient as defined in \eqnref{eqn:newtonLaG:gradient} works as expected, lets do a calculation that we know the answer as computed with an
orthonormal basis.

\begin{equation*}
f(\Br) = \norm{\Br}^\alpha
\end{equation*}

\begin{equation}\label{eqn:newtonianLagrangianAndGradient:80}
\begin{aligned}
\grad f(\Br)
&= \grad \norm{\Br}^\alpha \\
&= \sum \Be^k \PDb{} {x^k} {{\left(\sum x^i x^j g_{ij}\right)}^{\alpha/2}} \\
&= \frac{\alpha}{2} \sum \Be^k {{\left(\sum x^i x^j g_{ij}\right)}^{\alpha/2 -1}} \quad \PDb{}{x^k} {\left(\sum x^i x^j g_{ij}\right)} \\
&= \alpha \norm{\Br}^{\alpha-2} \sum \Be^k x^i g_{ki} \\
&= \alpha \norm{\Br}^{\alpha-2} \sum_i x^i
\mathLabelBox{\sum_k \Be^k \Be_k \cdot \Be_i}{\(=\Be_i\)}
\\
&= \alpha \norm{\Br}^{\alpha-2} \Br
\end{aligned}
\end{equation}
